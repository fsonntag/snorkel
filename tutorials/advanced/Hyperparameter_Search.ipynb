{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Searching for Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook, we'll demonstrate how to automatically select hyperparameters based on a labeled development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generating Some Data\n",
    "\n",
    "We'll generate some data from a known model of noisy labels in which two pairs of labeling functions are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModelWeights\n",
    "from snorkel.learning.structure import generate_label_matrix\n",
    "\n",
    "weights = GenerativeModelWeights(10)\n",
    "for i in range(10):\n",
    "    weights.lf_accuracy[i] = 2.5\n",
    "weights.dep_similar[0, 1] = 0.25\n",
    "weights.dep_similar[2, 3] = 0.25\n",
    "\n",
    "L_gold_train, L_train = generate_label_matrix(weights, 10000)\n",
    "L_gold_dev, L_dev = generate_label_matrix(weights, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generative Model Hyperparameters\n",
    "\n",
    "Below we use `RandomSearch` to search over a random set (with `n=5` below) of hyperparameter configurations, validating on the development set and saving the best model. Note we can also use the `GridSearch` class to search over _all_ combinations of hyperparameters. We start by defining the model, then the parameter ranges to search over, then the searcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "step_size_param = RangeParameter('step_size', 1e-6, 1e-2, step=1, log_base=10)\n",
    "decay_param     = ListParameter('decay', [1.0, 0.95, 0.9])\n",
    "epochs_param    = ListParameter('epochs', [20, 50, 100])\n",
    "\n",
    "searcher = RandomSearch(GenerativeModel, [step_size_param, decay_param, epochs_param], L_train, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we fit to the dev set--i.e. we execute the search, scoring and selecting the best configuration according to the dev set score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gen_model, run_stats = searcher.fit(L_dev, L_gold_dev)\n",
    "run_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the best learned model.\n",
    "\n",
    "**Note that this is just a toy example- the above search may not be that interesting (i.e. all models may get perfect scores). But in practice, hyperparameter tuning is extremely important, especially for the end discriminative model (see below).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that in other tutorials, this is often named `train_marginals`!\n",
    "Y_train = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Grid Search: `GridSearchMP`\n",
    "\n",
    "You can run `GridSearch` (and any subclasses like `RandomSearch`) in parallel just by specifying the `n_threads` keyword argument in the `fit` method!\n",
    "\n",
    "**Note: When running TensorFlow models in parallel, it is often necessary to throttle the number of threads that each individual model / process tries to use.  You can set that by settting `n_threads` in the initialization of the `GridSearch` object (here, this kwarg is for the discriminative model class).**\n",
    "\n",
    "*Note #2: There is currently an unknown issue with running parallel grid search in a notebook where other models have already been run (see [Issue #707](https://github.com/HazyResearch/snorkel/issues/707)), so recommended usage for now is to run in a separate notebook / from the command line.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Model Hyperparameters\n",
    "\n",
    "Hyperparameter search can also be used in the same way for the discriminative model, and in fact this is currently the most popular use of it! Below is example code:\n",
    "\n",
    "```\n",
    "from snorkel.learning import reRNN\n",
    "\n",
    "rate_param    = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "dropout_param = ListParameter('dropout', [0.0, 0.5])\n",
    "\n",
    "# We now add a session and probabilistic labels, as well as pass in the candidates\n",
    "# instead of the label matrix\n",
    "searcher = RandomSearch(reRNN, [rate_param, dropout_param], train_candidates, Y_train=Y_train, n=5)\n",
    "\n",
    "np.random.seed(1701)\n",
    "# We now pass in the development candidates and the gold development labels\n",
    "# Any arguments that should be passed to the training method for every trial can also be specified\n",
    "lstm, run_stats = searcher.fit(dev_candidates, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
